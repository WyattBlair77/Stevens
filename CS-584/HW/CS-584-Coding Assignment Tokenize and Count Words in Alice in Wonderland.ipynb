{"cells":[{"cell_type":"markdown","metadata":{},"source":["# M1.C4: Assignment: Python Coding \\#1\n","Wyatt Blair\n","\n","10/3/24\n","\n","___\n","\n","## Assignment\n","\n","The goal of this assignment is to thoroughly tokenize the text (.txt file) Alice in\n","Wonderland, which can be found in the ’Programming Assignment 1’ module,\n","and create two frequency dictionaries:\n","1. A Token Frequency Dictionary: Case-sensitive tokens based on speific tokenization rules.\n","\n","2. A Full Word Frequency Dictionary: Complete, correctly spelled words.\n","These will be python dictionaries. You must thoroughly comment on your\n","code, line-by-line, explaining new functions and what they do to process text.\n","You do NOT need to comment on well-known, trivial functions like the print\n","statement. However, you MUST explain how each regex or tokenization function\n","is changing the text.\n","Words in the ’Full Word Frequency Dictionary’ must be spelled correctly.\n","You will be graded at least partially on how correct and comprehensive this\n","dictionary is.\n","1\n","\n","## Output Requirements\n","\n","1. Token Frequency Dictionary\n","* Data Structure: Python dictionary.\n","* Keys: Case-sensitive tokens (e.g., \"Alice\", \"ALICE\", \"said\").\n","* Values: Frequency of each token (e.g., the number of occurrences).\n","* Punctuation: Include punctuation as standalone tokens.\n","* Example:\n","\"Alice\": 5, \"said\": 10, \"ca\": 3, \"n’t\": 3, \",\": 15\n","\n","2. Full Word Frequency Dictionary\n","* Data Structure: Python dictionary.\n","* Keys: Complete words.\n","* Values: Frequency of each complete word.\n","* Example:\n","\"Alice\": 5, \"can’t\": 3, \"believe\": 2\n","\n","## Instructions\n","\n","1. Tokenization:\n","* Tokenize the text of Alice in Wonderland using industry-standard rules:\n","– Split punctuation if it’s not part of the word (e.g., commas, periods).\n","– Keep contractions as separate tokens (e.g., \"ca\", \"n’t\").\n","– Maintain case sensitivity (e.g., \"Alice\" is distinct from \"ALICE\").\n","– Split hyphenated words unless they are common expressions or\n","proper nouns.\n","2. Dictionary Creation:\n","* Create a Token Dictionary that tracks the frequency of each token.\n","* Create a Full Word Dictionary that tracks the frequency of each\n","full word in the text (e.g., \"can’t\" remains one word in this dictionary).\n","\n","## Submission Instructions\n","\n","Submission Instructions\n","* Submit your assignment as a .py or .ipynb file.\n","* Upload your file to the Programming Assignment 1 section on the\n","course platform.\n","* I should be able to run your file by simply downloading it, and running it\n","in Google Colab.\n","* Include comments and explanations in your code of all unique regex, tokenization, and edit distance functions\n","* If you are using external libraries, include the installation commands at\n","the top of your file (e.g., !pip install <package>).\n","Due Date: October 16th, 2024\n","___"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import string"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["with open('../data/alice_in_wonderland.txt', 'r') as f:\n","    alice_text = f.read()\n","    alice_text = alice_text.replace('\\n', ' ')\n","    \n","    # remove any space greater than one\n","    while '  ' in alice_text:\n","        alice_text = alice_text.replace('  ', ' ')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def split_words(text):\n","\n","    # replacing un-used punctuation with empty strings\n","    blacklist = string.punctuation.replace(\"'\", \"\").replace(\"-\", \"\")\n","    text = text.translate(str.maketrans('', '', blacklist))\n","\n","    # split the text into words\n","    words: list[str] = [word for word in text.split(' ') if word]\n","\n","    return words"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def tokenize(text):\n","\n","    tokens = []\n","    for word in split_words(text):\n","\n","        new_tokens = []\n","        # split on apostrophe\n","        if \"'\" in word:\n","            \n","            punc_ind = word.index(\"'\")\n","            \n","            # if the apsostraphe is at the beginning or end of the word\n","            if punc_ind == 0 or punc_ind == len(word)-1:\n","                new_tokens.append(word.replace(\"'\", ''))\n","            # take letters on left and right of the apsostraphe\n","            else:\n","                left_token   = word[:punc_ind-1            ]\n","                center_token = word[ punc_ind-1: punc_ind+2]\n","                right_token  = word[ punc_ind+2:           ]\n","\n","                new_tokens.extend([token for token in [left_token, center_token, right_token] if token])\n","\n","        # split on hyphen\n","        elif \"-\" in word:\n","            \n","            punc_ind = word.index(\"-\")\n","            left_word = word[:punc_ind]\n","            right_word = word[punc_ind+1:]\n","\n","            # if the hyphen is at the beginning or end of the word\n","            if not left_word or not right_word:\n","                new_token = word.replace(\"-\", \"\")\n","                new_tokens.append(new_token)\n","\n","            # if both words are capitalized (proper noun)\n","            elif left_word[0].isupper() and right_word[0].isupper():\n","                new_tokens.append(word)\n","            \n","            # remove hyphen and add both words\n","            else:\n","                new_tokens.extend([left_word, right_word])\n","        \n","        # split on \"ing\"\n","        elif word[-3:].lower() == \"ing\":\n","\n","            left_token = word[:-3]\n","            right_token = word[-3:]\n","\n","            new_tokens.append(left_token)\n","            new_tokens.append(right_token)\n","\n","        # no punctuation-- add whole word as token\n","        else:\n","            new_tokens.append(word)\n","    \n","        tokens.extend(token for token in new_tokens if token)\n","    return tokens\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def token_frequency(tokens: list[str]) -> dict[str, int]:\n","    freq = {}\n","   \n","    for token in tokens:\n","        if token in freq:\n","            freq[token] += 1\n","        else:\n","            freq[token] = 1\n","\n","    return freq\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def word_frequency(text: str) -> dict[str, int]:\n","    \n","    clean_text = text.lower()\n","    words = split_words(clean_text)\n","\n","    freq = {}\n","    for word in words:\n","        if word in freq:\n","            freq[word] += 1\n","        else:\n","            freq[word] = 1\n","\n","    return freq"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["token_freq = token_frequency(tokenize(alice_text))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["word_freq = word_frequency(alice_text)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["PRINT_LIM = 100"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TOKEN FREQUENCY:\n","-> Alic: 10\n","-> e's: 52\n","-> Adventures: 3\n","-> in: 354\n","-> Wonderland: 3\n","-> ALIC: 3\n","-> E'S: 4\n","-> ADVENTURES: 1\n","-> IN: 2\n","-> WONDERLAND: 1\n","-> Lewis: 1\n","-> Carroll: 1\n","-> THE: 9\n","-> MILLENNIUM: 1\n","-> FULCRUM: 1\n","-> EDITION: 1\n","-> 30: 1\n","-> CHAPTER: 12\n","-> I: 424\n","-> Down: 3\n","-> the: 1534\n","-> Rabbit-Hole: 1\n","-> Alice: 386\n","-> was: 363\n","-> beginn: 11\n","-> ing: 851\n","-> to: 719\n","-> get: 44\n","-> very: 126\n","-> tired: 7\n","-> of: 494\n","-> sitt: 10\n","-> by: 54\n","-> her: 243\n","-> sister: 8\n","-> on: 190\n","-> bank: 3\n","-> and: 777\n","-> hav: 10\n","-> noth: 29\n","-> do: 123\n","-> once: 29\n","-> or: 75\n","-> twice: 4\n","-> she: 498\n","-> had: 184\n","-> peeped: 3\n","-> into: 67\n","-> book: 11\n","-> read: 13\n","-> but: 129\n","-> it: 483\n","-> no: 68\n","-> pictures: 4\n","-> conversations: 1\n","-> what: 90\n","-> is: 103\n","-> use: 18\n","-> a: 610\n","-> thought: 74\n","-> without: 26\n","-> conversation: 10\n","-> So: 27\n","-> consider: 4\n","-> own: 10\n","-> mind: 10\n","-> as: 246\n","-> well: 36\n","-> could: 82\n","-> for: 140\n","-> hot: 6\n","-> day: 26\n","-> made: 30\n","-> feel: 14\n","-> sleepy: 5\n","-> stupid: 5\n","-> whether: 11\n","-> pleasure: 2\n","-> mak: 8\n","-> daisy: 1\n","-> chain: 1\n","-> would: 82\n","-> be: 164\n","-> worth: 4\n","-> trouble: 6\n","-> gett: 21\n","-> up: 98\n","-> pick: 2\n","-> daisies: 1\n","-> when: 69\n","-> suddenly: 12\n","-> White: 22\n","-> Rabbit: 40\n","-> with: 175\n","-> pink: 1\n","-> eyes: 29\n","-> ran: 16\n","-> close: 13\n","-> There: 25\n","-> so: 124\n","-> VERY: 13\n","-> remarkable: 2\n"]}],"source":["print('TOKEN FREQUENCY:')\n","for i, (token, freq) in enumerate(token_freq.items()):\n","    print(f\"-> {token}: {freq}\")\n","    if i > PRINT_LIM: break"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TOKEN FREQUENCY (sorted):\n","-> the: 1534\n","-> ing: 851\n","-> and: 777\n","-> to: 719\n","-> a: 610\n","-> she: 498\n","-> of: 494\n","-> it: 483\n","-> said: 453\n","-> I: 424\n","-> Alice: 386\n","-> was: 363\n","-> in: 354\n","-> you: 311\n","-> that: 259\n","-> as: 246\n","-> her: 243\n","-> n't: 216\n","-> at: 200\n","-> on: 190\n","-> had: 184\n","-> with: 175\n","-> all: 171\n","-> be: 164\n","-> for: 140\n","-> but: 129\n","-> not: 128\n","-> very: 126\n","-> so: 124\n","-> little: 124\n","-> do: 123\n","-> out: 116\n","-> this: 113\n","-> The: 110\n","-> they: 107\n","-> t's: 105\n","-> is: 103\n","-> down: 99\n","-> up: 98\n","-> he: 97\n","-> about: 94\n","-> his: 94\n","-> one: 92\n","-> what: 90\n","-> them: 87\n","-> were: 86\n","-> know: 85\n","-> like: 84\n","-> e: 84\n","-> went: 83\n","-> again: 83\n","-> herself: 83\n","-> could: 82\n","-> would: 82\n","-> have: 81\n","-> if: 77\n","-> or: 75\n","-> thought: 74\n","-> go: 74\n","-> did: 73\n","-> then: 71\n","-> when: 69\n","-> no: 68\n","-> time: 68\n","-> into: 67\n","-> see: 67\n","-> And: 67\n","-> Queen: 67\n","-> say: 64\n","-> off: 62\n","-> me: 61\n","-> K: 60\n","-> look: 58\n","-> began: 58\n","-> think: 57\n","-> I'm: 57\n","-> Turtle: 57\n","-> l: 56\n","-> its: 56\n","-> Mock: 56\n","-> my: 55\n","-> Gryphon: 55\n","-> by: 54\n","-> Hatter: 54\n","-> quite: 53\n","-> your: 53\n","-> e's: 52\n","-> way: 52\n","-> an: 52\n","-> much: 51\n","-> their: 50\n","-> there: 49\n","-> head: 49\n","-> some: 48\n","-> who: 48\n","-> now: 47\n","-> more: 47\n","-> voice: 47\n","-> only: 46\n","-> looked: 45\n","-> got: 45\n","-> get: 44\n"]}],"source":["print('TOKEN FREQUENCY (sorted):')\n","for i, (token, freq) in enumerate(sorted(token_freq.items(), key=lambda x: x[1], reverse=True)):\n","    print(f\"-> {token}: {freq}\")\n","    if i > PRINT_LIM: break"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WORD FREQUENCY:\n","-> alice's: 13\n","-> adventures: 6\n","-> in: 363\n","-> wonderland: 4\n","-> lewis: 1\n","-> carroll: 1\n","-> the: 1630\n","-> millennium: 1\n","-> fulcrum: 1\n","-> edition: 1\n","-> 30: 1\n","-> chapter: 12\n","-> i: 396\n","-> down: 97\n","-> rabbit-hole: 3\n","-> alice: 383\n","-> was: 352\n","-> beginning: 11\n","-> to: 715\n","-> get: 46\n","-> very: 143\n","-> tired: 7\n","-> of: 505\n","-> sitting: 10\n","-> by: 56\n","-> her: 247\n","-> sister: 8\n","-> on: 183\n","-> bank: 2\n","-> and: 843\n","-> having: 10\n","-> nothing: 32\n","-> do: 70\n","-> once: 29\n","-> or: 75\n","-> twice: 4\n","-> she: 536\n","-> had: 177\n","-> peeped: 3\n","-> into: 67\n","-> book: 5\n","-> reading: 3\n","-> but: 164\n","-> it: 478\n","-> no: 84\n","-> pictures: 4\n","-> conversations: 1\n","-> what: 131\n","-> is: 92\n","-> use: 18\n","-> a: 626\n","-> book': 2\n","-> thought: 74\n","-> without: 25\n","-> conversation': 1\n","-> so: 142\n","-> considering: 3\n","-> own: 10\n","-> mind: 7\n","-> as: 261\n","-> well: 53\n","-> could: 77\n","-> for: 148\n","-> hot: 5\n","-> day: 21\n","-> made: 30\n","-> feel: 8\n","-> sleepy: 5\n","-> stupid: 4\n","-> whether: 11\n","-> pleasure: 2\n","-> making: 8\n","-> daisy-chain: 1\n","-> would: 82\n","-> be: 139\n","-> worth: 4\n","-> trouble: 5\n","-> getting: 21\n","-> up: 93\n","-> picking: 2\n","-> daisies: 1\n","-> when: 79\n","-> suddenly: 13\n","-> white: 30\n","-> rabbit: 42\n","-> with: 176\n","-> pink: 1\n","-> eyes: 27\n","-> ran: 16\n","-> close: 13\n","-> there: 73\n","-> remarkable: 2\n","-> that: 249\n","-> nor: 3\n","-> did: 58\n","-> think: 43\n","-> much: 44\n","-> out: 113\n","-> way: 49\n","-> hear: 14\n","-> say: 42\n","-> itself: 14\n"]}],"source":["print('WORD FREQUENCY:')\n","for i, (word, freq) in enumerate(word_freq.items()):\n","    print(f\"-> {word}: {freq}\")\n","    if i > PRINT_LIM: break"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WORD FREQUENCY (sorted):\n","-> the: 1630\n","-> and: 843\n","-> to: 715\n","-> a: 626\n","-> she: 536\n","-> of: 505\n","-> it: 478\n","-> said: 457\n","-> i: 396\n","-> alice: 383\n","-> in: 363\n","-> was: 352\n","-> you: 340\n","-> as: 261\n","-> that: 249\n","-> her: 247\n","-> at: 210\n","-> on: 183\n","-> had: 177\n","-> with: 176\n","-> all: 171\n","-> but: 164\n","-> for: 148\n","-> very: 143\n","-> so: 142\n","-> be: 139\n","-> not: 132\n","-> what: 131\n","-> this: 129\n","-> they: 127\n","-> little: 125\n","-> he: 119\n","-> out: 113\n","-> down: 97\n","-> his: 96\n","-> one: 95\n","-> if: 94\n","-> up: 93\n","-> is: 92\n","-> about: 92\n","-> no: 84\n","-> went: 83\n","-> herself: 83\n","-> were: 83\n","-> would: 82\n","-> have: 80\n","-> when: 79\n","-> then: 79\n","-> like: 78\n","-> could: 77\n","-> or: 75\n","-> thought: 74\n","-> there: 73\n","-> them: 73\n","-> do: 70\n","-> off: 69\n","-> again: 68\n","-> into: 67\n","-> queen: 65\n","-> how: 64\n","-> see: 62\n","-> your: 62\n","-> know: 61\n","-> who: 61\n","-> time: 60\n","-> king: 60\n","-> don't: 59\n","-> did: 58\n","-> my: 58\n","-> its: 57\n","-> an: 57\n","-> began: 57\n","-> i'm: 57\n","-> by: 56\n","-> mock: 56\n","-> quite: 55\n","-> turtle: 55\n","-> gryphon: 55\n","-> me: 54\n","-> it's: 54\n","-> hatter: 54\n","-> well: 53\n","-> their: 52\n","-> just: 51\n","-> some: 51\n","-> are: 51\n","-> way: 49\n","-> only: 49\n","-> which: 48\n","-> go: 48\n","-> get: 46\n","-> more: 46\n","-> it': 46\n","-> voice: 46\n","-> looked: 45\n","-> first: 45\n","-> come: 45\n","-> got: 45\n","-> much: 44\n","-> now: 44\n","-> think: 43\n","-> must: 43\n"]}],"source":["print('WORD FREQUENCY (sorted):')\n","for i, (word, freq) in enumerate(sorted(word_freq.items(), key=lambda x: x[1], reverse=True)):\n","    print(f\"-> {word}: {freq}\")\n","    if i > PRINT_LIM: break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
